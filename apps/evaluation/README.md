# SPARQL-QA Evaluation

This Streamlit application allows you to compare different question-to-SPARQL models across various knowledge graphs and datasets.

## Usage

1. Set `KG_BENCHMARK_DIR` env variable to `data/benchmark`: `export KG_BENCHMARK_DIR=$(pwd)/data/benchmark`
2. Navigate to the `apps/evaluation` directory
3. Install the required dependencies: `pip install -r requirements.txt`
4. Run the Streamlit app: `streamlit run app.py`

## Directory structure

The app expects data to be organized in the following format:

```
[KG_BENCHMARK_DIR]/
  [knowledge-graph]/             # Name of the knowledge graph
    [benchmark]/                 # Name of the benchmark
      test.jsonl                 # Benchmark input and ground truth
      outputs/
        [model1].jsonl           # Model output
        [model1].config.json     # Model config
        [model1].evaluation.json # Evaluation against ground truth
        ...
```

For example:

```
data/benchmark/
  wikidata/
    qald10/
      test.jsonl
      outputs/
        qwen-72b.search_extended.jsonl
        qwen-72b.search_extended.config.json
        qwen-72b.search_extended.evaluation.json
```

## Expected file formats

### Ground truth (test.jsonl)

Each line contains a JSON object like this, where `paraphrases` are paraphrases of the question and `info` contains arbitrary additional information:

```json
{
  "id": "Identifier",
  "question": "Question in natural language",
  "sparql": "SPARQL query answering the question",
  "paraphrases": [...],
  "info": {...}
}
```

### Model output (*.jsonl)

Each line contains a JSON objects like this, with optional additional fields:

```json
{
  "id": "Identifier",
  "sparql": "SPARQL query generated by the model",
  ...
}
```

### Evaluation (*.evaluation.json)

Model output evaluated against ground truth, as
produced by the `scripts/evaluate.py` script, in the format:

```json
{
  "Identifier": {
    "target": {
      "size": 3, # Size of ground truth SPARQL results
      "err": "Error occurring during ground truth SPARQL execution" | null
    },
    "prediction": {
      "sparql": "Predicted SPARQL query",
      "err": "Error occurring during predicted SPARQL execution" | null,
      "size": 4, # Size of predicted SPARQL results
      "score": 0.8, # F1-score between predicted and ground truth SPARQL results
      "elapsed": 10.3 # Time taken to predict the SPARQL query
    } | null # null if no prediction was made
  }
}
```
